# -*- coding: utf-8 -*-
"""nn-no-hidden-config1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SxVqQTwzeezDSXDNmOYeFtE1NBISnoe-
"""

# -*- coding: utf-8 -*-
"""
A "no hidden layer" bidirectional MNIST model, matching the paper's single-layer logic,
but using your function names (CustomDenseLayer, classification, generator, etc.).
At the end, we produce a summary table of results and save it as CSV.
"""

import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist

from google.colab import drive
drive.mount('/content/drive')

###############################################################################
# 1) Data Loading and Preprocessing
###############################################################################
def preprocess_data():
    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
    train_images = train_images.astype('float32') / 255.0
    test_images  = test_images.astype('float32')  / 255.0

    train_images = train_images.reshape([-1, 784])
    test_images  = test_images.reshape([-1, 784])

    # One-hot
    train_labels = tf.one_hot(train_labels, depth=10)
    test_labels  = tf.one_hot(test_labels,  depth=10)
    return train_images, train_labels, test_images, test_labels

###############################################################################
# 2) Single "No Hidden Layer" class
###############################################################################
class CustomDenseLayer(tf.keras.layers.Layer):
    def __init__(self, input_dim=784, output_dim=10):
        super(CustomDenseLayer, self).__init__()
        init_w = tf.random_normal_initializer(mean=0.0, stddev=0.1)
        self.weight = self.add_weight(
            shape=(input_dim, output_dim),
            initializer=init_w,
            trainable=True,
            dtype=tf.float32,
            name="C_W1"
        )
        self.bias = self.add_weight(
            shape=(output_dim,),
            initializer='zeros',
            trainable=True,
            dtype=tf.float32,
            name="C_B1"
        )
        self.inverse_bias = self.add_weight(
            shape=(input_dim,),
            initializer='zeros',
            trainable=True,
            dtype=tf.float32,
            name="G_B1"
        )

    def call(self, inputs):
        # Classifier forward pass: [batch, 784] -> [batch, 10]
        logits = tf.matmul(inputs, self.weight) + self.bias
        return logits

    def backward_pass(self, labels):
        # Generator backward pass: [batch, 10] -> [batch, 784]
        gen_logits = tf.matmul(labels, tf.transpose(self.weight)) + self.inverse_bias
        return gen_logits

###############################################################################
# 3) Instantiate Our Single Layer
###############################################################################
single_layer = CustomDenseLayer(input_dim=784, output_dim=10)

def classification(images):
    """
    Forward pass, returns [batch_size, 10] logits.
    """
    images = tf.reshape(images, [-1, 784])
    return single_layer(images)

def generator(labels):
    """
    Backward pass, returns [batch_size, 784] generator logits.
    """
    return single_layer.backward_pass(labels)

###############################################################################
# 5) FGSM Attack for Adversarial Examples
###############################################################################
def fgsm_attack(images, labels, epsilon=0.1):
    with tf.GradientTape() as tape:
        tape.watch(images)
        logits = classification(images)
        loss   = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
        loss   = tf.reduce_mean(loss)
    grads = tape.gradient(loss, images)
    signed_grads = tf.sign(grads)
    adv_images   = images + epsilon * signed_grads
    adv_images   = tf.clip_by_value(adv_images, 0.0, 1.0)
    return adv_images

###############################################################################
# 6) Accuracy / Loss Helpers
###############################################################################
def compute_accuracy(images, labels):
    logits = classification(images)
    preds  = tf.argmax(logits, axis=1)
    true   = tf.argmax(labels, axis=1)
    correct= tf.reduce_sum(tf.cast(tf.equal(preds, true), tf.float32))
    return (correct / float(images.shape[0])).numpy() * 100.0

def compute_class_loss(images, labels):
    logits = classification(images)
    return tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
    ).numpy()


def compute_gen_loss(images, labels):
    """
    We feed images -> classifier -> softmax -> generator -> gen_logits,
    then measure sigmoid cross-entropy with real images.
    """
    logits = classification(images)
    pred_prob  = tf.nn.softmax(logits, axis=1)
    gen_logits = generator(pred_prob)
    loss_gen   = tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(labels=images, logits=gen_logits)
    )
    return loss_gen.numpy()

def compute_r_sigmoid_and_r_softmax(images):
    """
    For logging: we often track the maximum activation in the output.
      r_sigmoid = max( sigmoid(logits) )
      r_softmax = max( softmax(logits) )
    We'll compute these on the entire input batch.
    """
    logits = classification(images)
    y_sig  = tf.nn.sigmoid(logits)
    y_soft = tf.nn.softmax(logits, axis=1)
    r_sigmoid = tf.reduce_max(y_sig)
    r_softmax = tf.reduce_max(y_soft)
    return r_sigmoid.numpy(), r_softmax.numpy()

###############################################################################
# 6) SEPARATE UPDATES: c_vars, g_vars
###############################################################################
# Paper code often uses:
# c_vars = [C_W1, C_B1] and g_vars = [C_W1, G_B1]
# so that classifier updates won't affect G_B1, and generator updates won't affect C_B1.
###############################################################################
def get_classifier_vars():
    return [single_layer.weight, single_layer.bias]

def get_generator_vars():
    return [single_layer.weight, single_layer.inverse_bias]

###############################################################################
# 4) Training Step (Classifier + Generator)
###############################################################################
def train_step(batch_x, batch_y, c_optimizer, g_optimizer):
    """
    1) Classifier update (c_vars only)
    2) Generator update (g_vars only)
    """
    # 1) CLASSIFIER UPDATE
    with tf.GradientTape() as tape_c:
        # Classification forward pass
        clf_logits = classification(batch_x)
        loss_classification = tf.reduce_mean(
            tf.nn.softmax_cross_entropy_with_logits(labels=batch_y, logits=clf_logits)
        )
    # Gradients wrt classifier vars only
    c_vars = get_classifier_vars()
    grads_c = tape_c.gradient(loss_classification, c_vars)
    c_optimizer.apply_gradients(zip(grads_c, c_vars))

    # 2) GENERATOR UPDATE
    with tf.GradientTape() as tape_g:
        # Use predicted softmax as input to generator
        pred_prob  = tf.nn.softmax(clf_logits, axis=1)
        gen_logits = generator(pred_prob)
        loss_generation = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(labels=batch_x, logits=gen_logits)
        )
    g_vars = get_generator_vars()
    grads_g = tape_g.gradient(loss_generation, g_vars)
    g_optimizer.apply_gradients(zip(grads_g, g_vars))

    return loss_classification, loss_generation

def train_model(train_images, train_labels, test_images, test_labels,
                num_steps=50000, batch_size=100, lr=0.003):
    """
    Step-based training with separate classifier & generator updates.
    Returns TF constants for train/test so we can build a final results table.
    """
    # Two separate optimizers (paper often uses same LR for both)
    c_optimizer = tf.optimizers.Adam(lr)
    g_optimizer = tf.optimizers.Adam(lr)

    # Convert data to TF
    train_x = tf.constant(train_images, dtype=tf.float32)
    train_y = tf.constant(train_labels, dtype=tf.float32)
    test_x  = tf.constant(test_images,  dtype=tf.float32)
    test_y  = tf.constant(test_labels,  dtype=tf.float32)

    num_train   = train_images.shape[0]
    num_batches = num_train // batch_size

    print("Starting Training with Full Biprop (classifier + generator each step)...")
    for step in range(num_steps + 1):
        batch_idx = step % num_batches
        start = batch_idx * batch_size
        end   = start + batch_size
        bx = train_x[start:end]
        by = train_y[start:end]

        lc, lg = train_step(bx, by, c_optimizer, g_optimizer)

        # Print logs every 1000 steps
        if step % 1000 == 0 or step == num_steps:
            print(f"Step {step:5d} | Class Loss: {lc.numpy():.4f} | Gen Loss: {lg.numpy():.4f} |",
                  f"Train Acc: {compute_accuracy(bx, by):.2f}%")

    print("\nTraining complete.")
    return train_x, train_y, test_x, test_y

def generate_results_table(train_x, train_y, test_x, test_y):
    table_rows = []

    # 1) Training (clean)
    train_acc = compute_accuracy(train_x, train_y)
    c_loss_train = compute_class_loss(train_x, train_y)
    g_loss_train = compute_gen_loss(train_x, train_y)
    table_rows.append({
        "Condition": "Training (Clean)",
        "Accuracy (%)": train_acc,
        "Classification Loss": c_loss_train,
        "Generation Loss": g_loss_train,
        "Noise Level / Epsilon": "-",
        "r_sigmoid": "-",
        "r_softmax": "-"
    })

    # 2) Test (clean)
    test_acc = compute_accuracy(test_x, test_y)
    c_loss_test = compute_class_loss(test_x, test_y)
    g_loss_test = compute_gen_loss(test_x, test_y)
    table_rows.append({
        "Condition": "Test (Clean)",
        "Accuracy (%)": test_acc,
        "Classification Loss": c_loss_test,
        "Generation Loss": g_loss_test,
        "Noise Level / Epsilon": "-",
        "r_sigmoid": "-",
        "r_softmax": "-"
    })

    # 3) Noisy
    for noise_level in [0.1, 0.2, 0.3]:
        rnd_noise = tf.random.uniform(shape=test_x.shape, minval=0., maxval=1., dtype=tf.float32)
        noisy_x   = tf.clip_by_value(test_x + noise_level*rnd_noise, 0., 1.)
        acc_noisy = compute_accuracy(noisy_x, test_y)
        c_loss_noisy = compute_class_loss(noisy_x, test_y)
        g_loss_noisy = compute_gen_loss(noisy_x, test_y)
        r_sig, r_soft = compute_r_sigmoid_and_r_softmax(noisy_x)

        table_rows.append({
            "Condition": f"Noisy {noise_level}",
            "Accuracy (%)": acc_noisy,
            "Classification Loss": c_loss_noisy,
            "Generation Loss": g_loss_noisy,
            "Noise Level / Epsilon": noise_level,
            "r_sigmoid": f"{r_sig:.6f}",
            "r_softmax": f"{r_soft:.6f}"
        })

    # 4) Adversarial
    for eps_val in [0.1, 0.3]:
        adv_x = fgsm_attack(test_x, test_y, epsilon=eps_val)
        acc_adv = compute_accuracy(adv_x, test_y)
        c_loss_adv = compute_class_loss(adv_x, test_y)
        g_loss_adv = compute_gen_loss(adv_x, test_y)
        r_sig, r_soft = compute_r_sigmoid_and_r_softmax(adv_x)

        table_rows.append({
            "Condition": f"Adversarial (Îµ={eps_val})",
            "Accuracy (%)": acc_adv,
            "Classification Loss": c_loss_adv,
            "Generation Loss": g_loss_adv,
            "Noise Level / Epsilon": eps_val,
            "r_sigmoid": f"{r_sig:.6f}",
            "r_softmax": f"{r_soft:.6f}"
        })

    df = pd.DataFrame(table_rows, columns=[
        "Condition",
        "Accuracy (%)",
        "Classification Loss",
        "Generation Loss",
        "Noise Level / Epsilon",
        "r_sigmoid",
        "r_softmax"
    ])
    return df

if __name__ == "__main__":
    from google.colab import drive
    drive.mount('/content/drive')

    # 1) Load data
    train_images, train_labels, test_images, test_labels = preprocess_data()

    # 2) Train model
    train_x, train_y, test_x, test_y = train_model(
        train_images, train_labels,
        test_images, test_labels,
        num_steps=50000,   # replicate paper
        batch_size=100,    # replicate paper
        lr=0.003
    )

    # 3) Generate final results table
    df_results = generate_results_table(train_x, train_y, test_x, test_y)
    print(df_results)
    df_results.to_csv("results_table.csv", index=False)
    print("Saved results_table.csv to disk!")

def visualize_reconstructions_three_parts(classification, generator, images, num_images=10):
    """
    Displays:
      Row 0: Original images (randomly selected)
      Row 1: Reconstructed from tf.eye(10) (digits 0..9)
      Row 2: Reconstructed from actual predicted labels of the selected images
    """
    # Ensure images are float32
    images = tf.cast(images, tf.float32)

    # 1) Select num_images at random
    shuffled_indices = np.random.choice(images.shape[0], size=num_images, replace=False)
    selected_images = tf.gather(images, shuffled_indices)

    # 2) Forward pass (classifier) on the selected images
    predictions = classification(selected_images)
    predicted_probs = tf.nn.softmax(predictions, axis=1)
    predicted_labels = tf.argmax(predicted_probs, axis=1)  # shape [num_images]

    # Convert predicted_labels to one-hot (for the generator)
    # We need shape=[num_images, 10] to feed the backward pass
    predicted_labels_one_hot = tf.one_hot(predicted_labels, depth=10)

    # 3) Generate from tf.eye(10) to see digits 0..9
    # We'll just take the first 'num_images' from that (or the entire 10 if you prefer)
    eye_10 = tf.eye(10)
    # If num_images < 10, we can slice eye_10 to shape [num_images, 10]
    eye_10 = eye_10[:num_images]

    reconstructed_from_eye = generator(eye_10)  # shape [num_images, 784]

    # 4) Generate from "actual predicted labels" for each selected image
    reconstructed_from_pred = generator(predicted_labels_one_hot)  # shape [num_images, 784]

    # 5) Set up subplots: 3 rows, num_images columns
    fig, axes = plt.subplots(nrows=3, ncols=num_images, figsize=(3 * num_images, 9))

    # Row 0: Original images
    for i in range(num_images):
        ax = axes[0, i]
        original_img = tf.reshape(selected_images[i], (28, 28))
        ax.imshow(original_img.numpy(), cmap='gray')
        ax.set_title('Original')
        ax.axis('off')

    # Row 1: Reconstructions from tf.eye(10)
    for i in range(num_images):
        ax = axes[1, i]
        recon_img = tf.reshape(reconstructed_from_eye[i], (28, 28))
        ax.imshow(recon_img.numpy(), cmap='gray')
        ax.set_title('From tf.eye(10)')
        ax.axis('off')

    # Row 2: Reconstructions from predicted labels
    for i in range(num_images):
        ax = axes[2, i]
        recon_img = tf.reshape(reconstructed_from_pred[i], (28, 28))
        ax.imshow(recon_img.numpy(), cmap='gray')
        ax.set_title('From Pred Label')
        ax.axis('off')

    plt.tight_layout()
    plt.show()
test_images = tf.cast(test_images, tf.float32)
visualize_reconstructions_three_parts(classification, generator, test_images)

print("train_labels shape:", train_labels.shape)
print("train_images shape is:", train_images.shape)

##########################
# 1) Load Data (MNIST)
##########################
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

# Suppose we want shape (60000,784) for images, shape (60000,) for labels
train_images = train_images.reshape([-1, 784]).astype('float32')
test_images  = test_images.reshape([-1, 784]).astype('float32')
train_labels = train_labels.astype('int32')
test_labels  = test_labels.astype('int32')

# Print shapes
print("train_images shape:", train_images.shape)  # e.g. (60000, 784)
print("train_labels shape:", train_labels.shape)  # e.g. (60000,)

##########################
# 2) Slice first 500
##########################
train_images_subset = train_images[:500]          # shape (500, 784)
train_labels_subset = train_labels[:500]          # shape (500,)

print("train_images_subset shape:", train_images_subset.shape)
print("train_labels_subset shape:", train_labels_subset.shape)

##########################
# 3) Possibly filter classes [0,1,2]
##########################
import numpy as np

selected_classes = [0, 1, 2]
indices = np.isin(train_labels_subset, selected_classes)

filtered_images = train_images_subset[indices]      # shape (X,784)
filtered_labels = train_labels_subset[indices]      # shape (X,)

print("filtered_images shape:", filtered_images.shape)
print("filtered_labels shape:", filtered_labels.shape)

##########################
# 4) PCA
##########################
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
filtered_images_2D = pca.fit_transform(filtered_images)

##########################
# 5) Classifier Logic
##########################
from sklearn.metrics import accuracy_score
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt

classifiers = {
    "Nearest Neighbors": KNeighborsClassifier(3),
    "Linear SVM": SVC(kernel="linear"),
    "RBF SVM": SVC(kernel="rbf", gamma=0.7),
    "Gaussian NB": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Neural Net": MLPClassifier(alpha=1, max_iter=1000),
    "Naive Bayes": GaussianNB(),
}

fig, axes = plt.subplots(2, 4, figsize=(13, 6))
ax = axes.ravel()

for i, (name, clf) in enumerate(classifiers.items()):
    # Train the classifier on the 2D data
    clf.fit(filtered_images_2D, filtered_labels)

    # Predict labels on the same data
    predictions = clf.predict(filtered_images_2D)
    accuracy = accuracy_score(filtered_labels, predictions) * 100

    # Display the decision boundary
    DecisionBoundaryDisplay.from_estimator(
        clf, filtered_images_2D, response_method="predict", ax=ax[i],
        cmap=plt.cm.RdYlBu, alpha=0.8
    )

    # Scatter plot
    scatter = ax[i].scatter(
        filtered_images_2D[:,0], filtered_images_2D[:,1],
        c=filtered_labels, edgecolor="k", cmap=plt.cm.RdYlBu, s=20
    )
    ax[i].set_title(f"{name}\nAccuracy: {accuracy:.2f}%")

plt.tight_layout()
plt.show()